{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import os as os\n",
    "import shutil\n",
    "import string\n",
    "import pickle\n",
    "# Helper libraries\n",
    "import collections\n",
    "import hashlib\n",
    "import nltk\n",
    "import json \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from rouge import Rouge\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.time = time.time()\n",
    "    def stop(self, description):\n",
    "        print(f'time taken for {description} is {time.time() - self.time}')\n",
    "        self.time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    #print(filename)\n",
    "    with open(filename,'r') as file:\n",
    "    # read all text\n",
    "        text = file.read()    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_story_highlight_diff_files(filename, story, highlights):\n",
    "    h_filename = filename.replace(\".story\", \".highlights\")\n",
    "    print(h_filename)\n",
    "    with open(filename,'w')as file:\n",
    "        story = \"@story: \\n\" + story\n",
    "        file.write(story)    \n",
    "    with open(h_filename, 'w') as h_file:\n",
    "        h_file.write(\"@abstract: \\n\")\n",
    "        highlights = list(map(lambda s: \"<s>\"+s+\"<\\s>\", highlights))\n",
    "        h_file.write(' '.join(highlights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_story_highlight(filename, story, highlights):\n",
    "    with open(filename,'a+') as file:\n",
    "        file.write(\"@abstract: \\n\")\n",
    "        highlights = list(map(lambda s: \"<s>\"+s+\"<\\s>\", highlights))\n",
    "        file.write(' '.join(highlights))                 \n",
    "        story = \"\\n@story: \\n\" + story\n",
    "        file.write(story)\n",
    "        file.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a document into news story and highlights\n",
    "def split_story(doc):\n",
    "    # find first highlight\n",
    "    index = doc.find('@highlight')\n",
    "    # split into story and highlights\n",
    "    story, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "    # strip extra white space around each highlight\n",
    "    highlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "    return story, highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "    stories = list()\n",
    "    rs_dir = \"./processed\"\n",
    "    if  os.path.exists(rs_dir):\n",
    "        shutil.rmtree(rs_dir)\n",
    "    os.makedirs(rs_dir)\n",
    "    list_of_files = listdir(directory)\n",
    "    count1 = -1\n",
    "        \n",
    "    for count,name in enumerate(list_of_files):\n",
    "        if \".story\" not in name:\n",
    "            continue\n",
    "        urlhash = os.path.splitext(name)\n",
    "        filename = directory + '/' + name\n",
    "        # load document\n",
    "        # print(filename)\n",
    "        doc = load_doc(filename)\n",
    "        # split into story and highlights\n",
    "        story, highlights = split_story(doc)\n",
    "        if(story.isspace()):\n",
    "            print(story+\" is empty!!!! \"+filename)\n",
    "            continue\n",
    "        if (count%1000 == 0):\n",
    "            count1 += 1\n",
    "            trainName = 'train'+ str(count1).zfill(3) \n",
    "        else:\n",
    "            trainName\n",
    "        fn = rs_dir + '/' + trainName\n",
    "        #write_story_highlight(fn, story, highlights)\n",
    "        # store\n",
    "        stories.append({'hash':urlhash[0], 'story':story, 'highlights':highlights})\n",
    "    return stories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_lines(lines):\n",
    "    cleaned = list()\n",
    "    # prepare a translation table to remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for line in lines:\n",
    "        # strip source cnn office if it exists\n",
    "        index = line.find('(CNN) -- ')\n",
    "        if index > -1:\n",
    "            line = line[index+len('(CNN)'):]\n",
    "        index = line.find('CNN')\n",
    "        if index > -1:\n",
    "            line = line[index+len('CNN'):]\n",
    "        # tokenize on white space\n",
    "        line = line.split()\n",
    "        # convert to lower case\n",
    "        line = [word.lower() for word in line]\n",
    "        # remove punctuation from each token\n",
    "        line = [w.translate(table) for w in line]\n",
    "        # remove tokens with numbers in them\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "        # store as string\n",
    "        cleaned.append(' '.join(line))\n",
    "    # remove empty strings\n",
    "    cleaned = [c for c in cleaned if len(c) > 0]\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_vocab(tokenList, corpus=None):\n",
    "    token_feed = [utils.canonicalize_word(w) for w in tokenList]\n",
    "    print(len(token_feed))\n",
    "    if corpus:\n",
    "        token_feed.extend([utils.canonicalize_word(w) for w in corpus.words()])\n",
    "        print(len(token_feed))\n",
    "    return token_feed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOUNS = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "def rank_sentences(sents, doc_matrix, map_feature_names, sentence_ordering=0, top_n=4):\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sents]\n",
    "    sentences = [[w for w in sent if nltk.pos_tag([w])[0][1] in NOUNS]\n",
    "                  for sent in sentences]\n",
    "    tfidf_sent = [[doc_matrix[map_feature_names[w.lower()]]\n",
    "                   for w in sent if w.lower() in map_feature_names]\n",
    "                 for sent in sentences]\n",
    "    #print(len(sents))\n",
    "    #print(len(tfidf_sent))\n",
    "    #print(len(sentences))\n",
    "\n",
    "    # Calculate Sentence Values\n",
    "    doc_val = sum(doc_matrix)\n",
    "    sent_values = [sum(sent) / doc_val for sent in tfidf_sent]\n",
    "    \n",
    "    #print(len(sent_values))\n",
    "    if sentence_ordering == 1:\n",
    "        #print(\"Coming to ordering 1\")\n",
    "        # Apply Position Weights\n",
    "        sent_values = [sent*(i/len(sent_values))\n",
    "                        for i, sent in enumerate(sent_values)]\n",
    "    elif sentence_ordering == 2:\n",
    "        #print(\"Coming to ordering 2\")\n",
    "        sent_values = [sent*((len(sent_values) - i)/len(sent_values))\n",
    "                        for i, sent in enumerate(sent_values)]\n",
    "        \n",
    "\n",
    "    ranked_sents = [pair for pair in zip(range(len(sent_values)), sent_values)]\n",
    "    ranked_sents = sorted(ranked_sents, key=lambda x: x[1] *-1)\n",
    "\n",
    "    return ranked_sents[:top_n]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_line_rouge_score(test_stories_list, test_highlights_list, n=4):\n",
    "    test_sentences = []\n",
    "    highlights = []\n",
    "    rouge_scores = []\n",
    "    for i in range(len(test_stories_list)):\n",
    "        test_sentences.append(' '.join(test_stories_list[i][:n]))\n",
    "        highlights.append(' '.join(test_highlights_list[i]))\n",
    "        rouge = Rouge()\n",
    "        rg_score = rouge.get_scores(test_sentences[i],highlights[i])\n",
    "        rouge_scores.append(rg_score)\n",
    "    \n",
    "        #print(rouge.get_scores(t[i],h[i]))\n",
    "        if(i%400==0):\n",
    "            print(i)\n",
    "            print(t[i]+'\\n\\n'+h[i])\n",
    "            print(\"\\n\\ntfidf_score: \",tfidf_s[i])\n",
    "    return rouge_scores\n",
    "\n",
    "def get_rouge_score(test_stories_list, test_highlights_list, count_vect, tfidf, sentence_ordering=0, n=4):\n",
    "    timer = Timer()\n",
    "    test_sentences = []\n",
    "    highlights = []\n",
    "    tfidf_s = []\n",
    "    rouge_scores = []\n",
    "    len_stories = len(test_stories_list)\n",
    "    map_feature_names = {word:index for index,word in enumerate(count_vect.get_feature_names())}\n",
    "    for i in range(len_stories):\n",
    "        story_freq_term = count_vect.transform(test_stories_list[i])\n",
    "        story_tfidf_matrix = tfidf.transform(story_freq_term)\n",
    "        story_dense = story_tfidf_matrix.todense()\n",
    "        doc_matrix = story_dense.tolist()[0]\n",
    "        rank = rank_sentences(test_stories_list[i], doc_matrix, map_feature_names, sentence_ordering, n)\n",
    "        test_sentences.append(' '.join([test_stories_list[i][id[0]] for id in rank]))\n",
    "        tfidf_s.append([id[1] for id in rank])\n",
    "        h1 = ' '.join(test_highlights_list[i])\n",
    "        rouge = Rouge()\n",
    "        rg_score = rouge.get_scores(test_sentences[i],h1)\n",
    "        rouge_scores.append(rg_score)\n",
    "        #print(rouge.get_scores(t[i],h[i]))\n",
    "        if(i%400==0):\n",
    "            timer.stop(f'time taken for processing {i+1} sentences')\n",
    "            print(i)\n",
    "            print(test_sentences[i]+'\\n\\n'+h1)\n",
    "            print(\"\\n\\ntfidf_score: \",tfidf_s[i])\n",
    "    return rouge_scores, test_sentences, tfidf_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeresult_file(r_scores_tmp, filename):\n",
    "    scores = [[score_type for rouge_type in score[0].values() for score_type in rouge_type.values()] for score in r_scores_tmp]\n",
    "    df = pd.DataFrame(scores, columns=[key1+'_'+key2 for dict1 in r_scores_tmp[0] for key1 in dict1.keys() for key2 in dict1[key1]])\n",
    "    df.to_csv(filename + '.csv')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def makeprocessed_file(t, hash_list, highlight_list):\n",
    "    for i,story in enumerate(t):\n",
    "        with open('cnn/stories/processed/'+test_hash_list[i]+'.story', 'w') as pfile:\n",
    "            pfile.write(string(story))\n",
    "            pfile.write(\"\\n\")\n",
    "            for highlight in test_highlight_list[i]:\n",
    "                pfile.write('@highlight\\n')\n",
    "                pfile.write(highlight)\n",
    "                pfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset):\n",
    "        return word\n",
    "    else:\n",
    "        return constants.UNK_TOKEN\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]\n",
    "\n",
    "#\n",
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we need to load up the data, the data can be found at CNN data link. We untar it in the same directory : tar xvf cnn_stories.tgz on the command line. After we load up the data, we separate out to the story and highlight portion and store it as a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shrividyamanmohan/Capstone_Project'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stories\n",
    "try:\n",
    "    file = open('cnn/stories/stories.json')\n",
    "    stories = json.load(file)\n",
    "    file.close()\n",
    "except:\n",
    "    directory = 'cnn/stories'\n",
    "    stories = load_stories(directory)\n",
    "    print('Loaded Stories %d' % len(stories))\n",
    "    # clean stories\n",
    "    for i,example in enumerate(stories):\n",
    "        example['story'] = clean_lines(example['story'].split('\\n'))\n",
    "        example['highlights'] = clean_lines(example['highlights'])\n",
    "        stories[i] = example\n",
    "    with open('cnn/stories/stories.json', 'w') as outfile:\n",
    "        json.dump(stories, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92465"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('cnn/stories/stories.json')\n",
    "stories1 = json.load(file)\n",
    "file.close()\n",
    "len(stories1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now splitting to an array of story sentences and the corresponding highlight sentences we get :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_list = list(map(lambda s: s['story'], stories))\n",
    "highlights_list = list(map(lambda s: s['highlights'], stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_story_length = np.mean(list(map(lambda s: len(s), stories_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sentence_length_of_every_story = np.mean(list(map(lambda s: len(s), [s for t in stories_list for s in t])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of story is 21.241 sentences and the Mean sentence length of every story is 171.595 charachters\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean length of story is {:.3f} sentences and the Mean sentence length of every story is {:.3f} charachters\".format(mean_story_length, mean_sentence_length_of_every_story))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now separating the datasets to training, dev and test datasets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_stories(stories, highlights, split=0.8, shuffle=False):\n",
    "    \"\"\"Generate train/test split for unsupervised tasks.\n",
    "\n",
    "    Args:\n",
    "      stories(list): list of stories\n",
    "      split (double): fraction to use as training set\n",
    "      shuffle (int or bool): seed for shuffle of input data, or False to just\n",
    "      take the training data as the first xx% contiguously.\n",
    "\n",
    "    Returns:\n",
    "      train_sentences, test_sentences ( list(list(string)) ): the train and test\n",
    "      splits\n",
    "    \"\"\"\n",
    "    sentences = np.array(list(stories), dtype=list)\n",
    "    fmt = (len(sentences), sum(map(len, sentences)))\n",
    "    print(\"Loaded {:,} stories ({:g} sentences)\".format(*fmt))\n",
    "\n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(shuffle)\n",
    "        all_sents = list(zip(sentences, highlights))\n",
    "        rng.shuffle(all_sents)\n",
    "        sentences, highlights = zip(*all_sents)\n",
    "       # rng.shuffle(sentences)  # in-place\n",
    "       # rng.shuffle(highlights)\n",
    "    split_idx = int(split * len(sentences))\n",
    "    test_dev_split_idx = int((len(sentences) - split_idx)/2)+ split_idx\n",
    "    print(split_idx, test_dev_split_idx)\n",
    "    train_stories = sentences[:split_idx]\n",
    "    dev_stories = sentences[split_idx:test_dev_split_idx]\n",
    "    test_stories = sentences[test_dev_split_idx:]\n",
    "    train_highlights = highlights[:split_idx]\n",
    "    dev_highlights = highlights[split_idx:test_dev_split_idx]\n",
    "    test_highlights = highlights[test_dev_split_idx:]\n",
    "    \n",
    "    \n",
    "    fmt = (len(train_stories), sum(map(len, train_stories)))\n",
    "    print(\"Training set: {:,} stories ({:,} sentences)\".format(*fmt))\n",
    "    fmt = (len(dev_stories), sum(map(len, dev_stories)))\n",
    "    print(\"Dev set: {:,} stories ({:,} sentences)\".format(*fmt))\n",
    "    fmt = (len(test_stories), sum(map(len, test_stories)))\n",
    "    print(\"Test set: {:,} stories ({:,} sentences)\".format(*fmt))\n",
    "\n",
    "    return train_stories, dev_stories, test_stories, train_highlights, dev_highlights, test_highlights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 92,465 stories (1.96402e+06 sentences)\n",
      "83218 87841\n",
      "Training set: 83,218 stories (1,766,718 sentences)\n",
      "Dev set: 4,623 stories (99,117 sentences)\n",
      "Test set: 4,624 stories (98,180 sentences)\n"
     ]
    }
   ],
   "source": [
    "train_stories_list, dev_stories_list, test_stories_list, train_highlights_list, dev_highlights_list, test_highlights_list \\\n",
    "= get_train_test_stories(stories_list , highlights_list, split=0.9, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the end is closer than the beginning',\n",
       " 'ron rupert grint left harry daniel radcliffe second from right and hermione emma watson in the new film',\n",
       " 'harry potter and his friends at hogwarts are now in their sixth year of seven at the school theyve seen a lot of changes particularly as the influence of the reawakened voldemort that is he who must not be named has made itself known',\n",
       " 'and the movie series itself is now nearing its conclusion harry potter and the halfblood prince which comes out wednesday is likewise the sixth movie in the series based on jk rowlings seven harry potter books',\n",
       " 'there is one benefit to having such history director david yates says pretty much everybody going to see halfblood prince is familiar with the characters whether through the books or the movies',\n",
       " 'we made a decision we kind of crossed a line actually i think on this movie where we said you know this is the sixth one in the series its the most popular franchise probably in history do we stop and explain things to the audience who may have not seen any of the others and we said no yates said and you know why its because they can always go back to dvds they can go back to the books',\n",
       " 'indeed many fans know the books and the movies backward and forward interactive harry potter',\n",
       " 'the new film reflects the growth of the characters harry the orphaned boy wizard who has been forced to take on responsibilities beyond his youthful years ron weasley rupert grint his redhaired best friend who is finding depths of courage in himself he wasnt aware of hermione granger emma watson the bookish and indispensible member of their clan who has demonstrated key leadership qualities and all the rest preparing for the showdown with the archvillain voldemort watch the potter cast answer your questions',\n",
       " 'among the returning performers michael gambon as dumbledore alan rickman as snape maggie smith as mcgonagall robbie coltrane as hagrid and ralph fiennes as voldemort one addition is jim broadbent who plays horace slughorn a former professor brought back to hogwarts watch the stars at the rainy premiere',\n",
       " 'despite the growing darkness theres also a lightness to the new film says daniel radcliffe who plays harry unlike the previous two movies in the series which were rated this one is rated a more familyfriendly pg',\n",
       " 'i think this films funnier radcliffe said there are a couple of moments which i laughed out loud at',\n",
       " 'not that its going to be a barrel of laughs yates cautions how could it with the snakefaced voldemort growing ever stronger',\n",
       " 'it is a bit bipolar yates said on the one hand theres all this romance and on the other hand people are getting killed and bridges are being blown up',\n",
       " 'oh yes theres romance after all the main potter characters are all teenagers now with all the teenage longings',\n",
       " 'is a unit of time warner',\n",
       " 'warner bros said at the time that the film perfectly fills the gap for a major tentpole release for midsummer and added that the delay was also due to repercussions from the writers strike',\n",
       " 'but fans werent so easily placated filling message boards with angry comments and starting petitions that garnered tens of thousands of signatures warner bros president alan horn was even moved to put out a statement assuring fans that the scheduling change was not taken lightly',\n",
       " 'now that the moment is finally at hand the fans seem to have forgiven the studio according to fandangocom a movie ticketselling site harry potter and the halfblood prince is outselling transformers revenge of the fallen at the same point in the sales cycle its also in movieticketscoms top advance sellers of all time',\n",
       " 'given that transformers is by far the years topgrossing film those tidbits cant help but make the studio happy ireportcom seeing the latest potter share your review',\n",
       " 'the last of rowlings potter books harry potter and the deathly hallows is in production now as two films thats given the cast a bit of temporal whiplash when talking about halfblood prince since they completed it more than a year ago but theyve been game to talk',\n",
       " 'after all theyve come a long way since harry potter and the sorcerers stone way back in and theyre aware as anyone of the passage of time that theyre closer to the end than the beginning',\n",
       " 'its the kind of situation that leads to sentimental reflections but watson for one couldnt wait to let go of one thing her school uniform',\n",
       " 'i was like burn it she told entertainment weekly oh my god to be done with those shoes and that uniform that was an exciting day']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stories_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_sentences = [item for sublist in train_stories_list for item in sublist]\n",
    "all_dev_sentences = [item for sublist in dev_stories_list for item in sublist]\n",
    "all_highlights_sentences = [item for sublist in train_highlights_list+dev_highlights_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list = all_train_sentences + all_dev_sentences + all_highlights_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shrividyamanmohan/Library/Python/3.7/lib/python/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['english'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken for Done with preprocessing is 141.38331484794617\n"
     ]
    }
   ],
   "source": [
    "timer = Timer()\n",
    "count_vect = CountVectorizer(preprocessor=canonicalize_word, stop_words={'English'})\n",
    "count_vect = count_vect.fit(combined_list)\n",
    "#freq_term_matrix = count_vect.transform(train_stories_list[0])\n",
    "freq_term_matrix = count_vect.transform(combined_list)\n",
    "feature_names = count_vect.get_feature_names()\n",
    "map_feature_names = {word:index for index,word in enumerate(count_vect.get_feature_names())}\n",
    "timer.stop('Done with preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shrividyamanmohan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('averaged_perceptron_tagger')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shrividyamanmohan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 15 21:53:42 2020\n",
      "time taken for time taken for processing 1 sentences is 0.5898070335388184\n",
      "0\n",
      "chef gordon ramsay writes of seeing his father abuse his mother today ramsay fights domestic abuse domestic violence is not identified solely by violent physical abuse instead it is defined as physical sexual psychological financial or emotional violence that takes place in a relationship intimate or familyoriented eventually this develops into a pattern of coercive and controlling behavior to this day i will never understand why mum stayed with him she deserved so much better and so much more it still pains me to remember how badly he treated her i have four young children of my own and i could never see myself behaving the way my father did when i was a child i want to be a role model for my children and have them look up to me no child should ever have to live in fear in their own home a home should be a place where you feel safe and loved when i was a kid our home was anything but that\n",
      "\n",
      "gordon ramsay says his father battled alcohol and abused ramsays mom ramsay says in women in the uk experience domestic violence he tries to fight the problem by speaking out and raising money\n",
      "\n",
      "\n",
      "tfidf_score:  [0.017226147966768603, 0.01356436633076698, 0.008674381662839604, 0.003469752665135842]\n",
      "time taken for time taken for processing 401 sentences is 122.44739890098572\n",
      "400\n",
      "heartbroken and baffled thats how friends and relatives of a mississippi family described themselves the day authorities said theyd found the bodies of atira hill laterry smith and jaidon hill the mother stepfather and boy vanished last week we went to bed last night still praying they would be brought home safely but unfortunately that did not happen said vinson jenkins hills cousin to my knowledge we dont know why anybody would want to do any harm to them\n",
      "\n",
      "the bodies of atira hill laterry smith and jaidon hill were found overnight the family was last seen friday in a car that vehicle was later found flipped and on fire authorities have arrested timothy burns on suspicion of arson a sheriff says\n",
      "\n",
      "\n",
      "tfidf_score:  [0.0, 0.0, 0.0, 0.0]\n",
      "time taken for time taken for processing 801 sentences is 123.84097290039062\n",
      "800\n",
      "the us fish and wildlife service says brown tree snakes have wiped out most of guams native populations of forest birds since being accidentally introduced to the island after world war ii probably after they stowed away on a ship or plane from their native range in australia papua new guinea and the solomon islands the snakes which can grow up to feet long also eat small mammals and lizards the goal he said is eradicating the brown tree snake from the island the dead mice will target snakes in a fencedin area of the base he said so officials will be able to determine the effectiveness in that area versus an adjacent area that the snakes could move in and out of the snake is mildly venomous the fws says but it does not pose a danger to most adults and its bite will not penetrate clothing\n",
      "\n",
      "dead mice to be laced with acetaminophen drug can kill snakes in hours brown tree snake has wiped out guams bird populations up to million snakes may be on island\n",
      "\n",
      "\n",
      "tfidf_score:  [0.05969730857002742, 0.04184537323924959, 0.029533061681414893, 0.02482919279374339]\n",
      "time taken for time taken for processing 1201 sentences is 106.2375807762146\n",
      "1200\n",
      "those uniforms during its heyday blockbuster had disneystyle cleancut rules for its employees who also had to wear pretty dorky uniforms rather than dress like you know normal people like at indie video and record stores a group of male employees actually sued blockbuster for a policy that banned male employees from having long hair check out this blockbuster employee training video below a creepily omniscient training manager who calls himself a professional opportunist gets rather inappropriate with a teenage clerk and prods her to date a customers son while harassing her for apparently committing the unspeakable sin of accepting bon jovi tickets if you find yourself obsessing about this amazing video which is totally understandable youll find a breakdown of it from an exemployee here the wrong movie is in the box these first few entries listed could be said about video stores in general but once blockbuster video conquered the known video rental world blockbuster video was the video store the company became synonymous with what a video store is and its downfall is largely because it kept being just a video store so these are fair game so you get home from blockbuster you have your overpriced soda and movietheatersized box of skittles and your movie you go to the dvd player and open the box and then where is starship troopers this is flatliners late fees of course late fees once you rented a blockbuster video the clock was ticking you better watch it the first night especially if it was a coveted new release those dollaraday fees stack up quick and theres an unholy checkingaccountcrippling full replacement fee lurking if you ever actually lost one of those boxes for all their brightly lit stores blockbuster video operated like the grim gang in goodfellas oh you forgot to return a dvd by midnight fk you pay me you returned it with the wrong disc fk you pay me indieforeign film ha its called blockbuster video for a reason film geek you want an art film or something with something nutty like subtitles you best go someplace else but blockbuster will have copies of enemy of the state\n",
      "\n",
      "blockbuster is closing its remaining stores some of the things ew wont miss includes the wrong movie in the box ew also wont miss the uniforms\n",
      "\n",
      "\n",
      "tfidf_score:  [0.15533328739643715, 0.1450875898478407, 0.08018676274168571, 0.04097862602419935]\n",
      "time taken for time taken for processing 1601 sentences is 117.56369590759277\n",
      "1600\n",
      "managing director flavio briatore personally wishes to state criminal proceedings against nelson piquet junior and nelson piquet senior have commenced in france concerning the making of false allegations the statement read piquet attributed the crash to a simple error at the time formula one supremo bernie ecclestone recently warned there is going to be a lot of trouble if the allegations are found to be true renault who dismissed piquet jr as their driver in august confirmed they have referred the matter to the british police\n",
      "\n",
      "renault commence legal action against nelson piquet jr over racefixing claim team boss flavio briatore also accuses driver of blackmailing renault allegations refer to fernando alonsos singapore grand prix victory renault to face findings of fia investigation into incident on september\n",
      "\n",
      "\n",
      "tfidf_score:  [0.09241635754135125, 0.06917359899761479, 0.05881660892827703, 0.05375237674758281]\n",
      "time taken for time taken for processing 2001 sentences is 118.66617012023926\n",
      "2000\n",
      "freaky aliens and endoftheworld scenarios are often catnip to the scifi fan community something executive producer justin falvey knows all too well falling skies executive producer darryl frank knows what scifi fans have been missing and said it was a specific design of the show to get writers who could offer compelling stories its one reason why science fiction fans love the genre so much stories that envision the future can focus on how society evolves and what fantastic technology could exist like gene roddenberrys vision in star trek but they can also show a more directly relatable side of future humanity something steven spielberg spoke about from the beginning is it should feel incredibly authentic and incredibly grounded in real human emotions people should be able to posit the question what would you do if you were in this scenario he said\n",
      "\n",
      "apocalyptic science fiction focuses on aliens and the human condition fans wonder why science fiction shows seem to be first on the network chopping blocks falling skies premiers sunday at pm est\n",
      "\n",
      "\n",
      "tfidf_score:  [0.31848052770301233, 0.20265112072630106, 0.1547010366284419, 0.12243761848188817]\n",
      "time taken for time taken for processing 2401 sentences is 118.92804002761841\n",
      "2400\n",
      "contributor and former adviser to president clinton said that vacationing or not the president is the president wherever he is the problem for obama stanzel said is the visuals that could come out of his trip a picture of obama playing golf alongside images from the gulf could send a negative message of all of the concerns that americans may have they do not need to worry whether president obama is a hardworking man they may agree or disagree with his policies but there is just no doubt that the guy is busting his rear end i thought it was silly when people attacked bush for going on vacation so ill be consistent and say its silly when people attack president obama for going on vacation he said\n",
      "\n",
      "obama and family at mount desert island maine for the weekend gop criticizing obama for vacationing during environmental disaster democratic strategist says criticism is galling considering bushs frequent trips the president is the president wherever he is paul begala says\n",
      "\n",
      "\n",
      "tfidf_score:  [0.12266090594641166, 0.07138986002190423, 0.07079876588156214, 0.06686550111036425]\n",
      "time taken for time taken for processing 2801 sentences is 114.6737630367279\n",
      "2800\n",
      "agerelated conditions including macular degeneration and hearing loss have kept graham at home in recent years but the evangelist is involved in writing projects including a manuscript for a book about aging ross said graham has been hospitalized or treated for several ailments over the years including a head surgery a fall intestinal bleeding prostate cancer and parkinsons disease according to ross graham has remained in good health overall grahams wife ruth bell graham died in during his hospital stay graham watched television and remained in good spirits spokesman a larry ross said he was visited wednesday evening by his eldest daughter gigi and met with his pastor dr don wilton thursday morning\n",
      "\n",
      "his physician says graham has responded very well to treatment the minister had developed pneumonia graham is expected to continue his recovery at home he has preached to millions over six decades\n",
      "\n",
      "\n",
      "tfidf_score:  [0.1162129995837308, 0.06540105784322181, 0.05232084627457744, 0.04877707968810483]\n",
      "time taken for time taken for processing 3201 sentences is 128.9990839958191\n",
      "3200\n",
      "local volunteers have joined police and rescue teams in searching the town and surrounding area a task made more difficult by poor weather flooding rivers and hilly terrain police dogs coast guard teams kayakers and mountain rescue specialists have been called in to assist in the search we are desperate for any news april is only years old please please help us find her it has been hours since our april was taken from us she said breaking down in tears there must be someone out there who knows where she is and can help the police find her\n",
      "\n",
      "police say they will question suspect mark bridger again hundreds of police volunteers and mountain rescue workers are looking for the prime minister david cameron also appeals for anyone with information to contact police april was snatched from the street on monday as she played with a friend\n",
      "\n",
      "\n",
      "tfidf_score:  [0.1084635929413917, 0.09969343228146416, 0.057849232259966285, 0.04030625100260738]\n",
      "time taken for time taken for processing 3601 sentences is 111.32203483581543\n",
      "3600\n",
      "mexico state is bordered on the west by michoacan and adjoins mexico city on three sides north east and west on sunday mexican president felipe calderon toured valle de chalco another city in mexico state on the eastern outskirts of the mexico city metro area in addition to michoacan and mexico states unusually heavy rain in the past week also flooded parts of mexico city the nations capital the death toll in michoacan had been until three additional bodies were discovered tuesday\n",
      "\n",
      "most of deaths have been in mexicos eastern michoacan state a dozen people missing after mudslides michoacan governor says flooding has affected up to people nationwide officials say\n",
      "\n",
      "\n",
      "tfidf_score:  [0.14556851485370317, 0.09704567656913546, 0.07763654125530836, 0.06777444992505043]\n",
      "time taken for time taken for processing 4001 sentences is 113.06687903404236\n",
      "4000\n",
      "earlier englishs mother and stepfather told police that they typically talked with her several times a week and became worried after she did not answer their calls since they last talked with her on christmas they eventually used a spare key to enter her apartment where they saw no sign of english the police report said englishs abandoned car was discovered with the motor running in atlanta in late december investigators used dental records to identify a body found in southwest atlanta as that of stacey nicole english the fulton county medical examiners office said in a statement a woman who was reported missing shortly after christmas has been found dead in atlanta authorities said wednesday\n",
      "\n",
      "new toxicology reports will provide important info private investigator says stacey nicole english was reported missing after christmas investigators used dental records to identify her body the medical examiners office says it has not determined a cause of death\n",
      "\n",
      "\n",
      "tfidf_score:  [0.06688340158274592, 0.021158392834298167, 0.012177368605328583, 0.0]\n",
      "time taken for time taken for processing 4401 sentences is 110.31789708137512\n",
      "4400\n",
      "while condemning the grave abuses committed by boko haram amnesty international said nigerias security forces have perpetrated serious human rights violations in their response they include strengthening systems and training to ensure abuses are not carried out by security forces and greater efforts to investigate abuse allegations and ensure the safety of witnesses from its base in muslimdominated northern nigeria boko haram which has referred to itself as the nigerian taliban has waged a violent campaign of bombings of christian churches that have killed hundreds and wounded many more there have been fewer attacks by christian militant groups the failure of the government to properly equip and train the police reduces their ability to ensure their own and other peoples safety hundreds of police military and sss state security service have been killed by boko haram since they have been shot and blown up in their stations at roadblocks and in their homes the report said\n",
      "\n",
      "new nigerias military denies abuses says rights group allegations are unfounded rights group nigerias security forces are committing abuses as they combat militants amnesty international says the government must do more to protect the public boko haram militants have killed hundreds in attacks on civilian targets and police\n",
      "\n",
      "\n",
      "tfidf_score:  [0.14147559824845735, 0.139280609434227, 0.13583215453763078, 0.1286148362970531]\n",
      "Fri May 15 22:16:10 2020\n",
      "time taken for Done with processing is 1348.0071280002594\n"
     ]
    }
   ],
   "source": [
    "timer = Timer()\n",
    "tfidf = TfidfTransformer(norm = 'l2')\n",
    "tfidf.fit(freq_term_matrix)\n",
    "#tfidf.fit(freq_term_matrix)\n",
    "print(time.ctime())\n",
    "r_score, t, tfidif_scores = get_rouge_score(test_stories_list, test_highlights_list, count_vect, tfidf, sentence_ordering=1, n=4)\n",
    "print(time.ctime())\n",
    "timer.stop('Done with processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
